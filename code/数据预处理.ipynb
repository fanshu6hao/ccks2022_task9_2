{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526d2f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "from csv import DictWriter\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import accumulate\n",
    "import re\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f09af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换jsonl为csv文件\n",
    "datafolder = './data'\n",
    "\n",
    "with open(os.path.join(datafolder, 'item_train_info.jsonl'), 'r') as inp, open(os.path.join(datafolder, 'train_info.csv'), 'w') as outp:\n",
    "    writer = DictWriter(outp, fieldnames=[\n",
    "            'item_id', 'industry_name', 'cate_id',\n",
    "        'cate_name', 'cate_id_path', 'cate_name_path', 'item_image_name', 'title', 'item_pvs', 'sku_pvs'])\n",
    "    for line in inp:\n",
    "        row = json.loads(line)\n",
    "        writer.writerow(row)\n",
    "        \n",
    "# with open(os.path.join(datafolder, 'item_valid_info.jsonl'), 'r') as inp, open(os.path.join(datafolder, 'valid_info.csv'), 'w') as outp:\n",
    "#     writer = DictWriter(outp, fieldnames=[\n",
    "#             'item_id', 'industry_name', 'cate_id',\n",
    "#         'cate_name', 'cate_id_path', 'cate_name_path', 'item_image_name', 'title', 'item_pvs', 'sku_pvs'])\n",
    "#     for line in inp:\n",
    "#         row = json.loads(line)\n",
    "#         writer.writerow(row)\n",
    "        \n",
    "        \n",
    "with open(os.path.join(datafolder, 'item_test_info.jsonl'), 'r') as inp, open(os.path.join(datafolder, 'test_info.csv'), 'w') as outp:\n",
    "    writer = DictWriter(outp, fieldnames=[\n",
    "            'item_id', 'industry_name', 'cate_id',\n",
    "        'cate_name', 'cate_id_path', 'cate_name_path', 'item_image_name', 'title', 'item_pvs', 'sku_pvs'])\n",
    "    for line in inp:\n",
    "        row = json.loads(line)\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde454d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换jsonl为csv文件\n",
    "datafolder = './data'\n",
    "\n",
    "with open(os.path.join(datafolder, 'item_train_pair.jsonl'), 'r') as inp, open(os.path.join(datafolder, 'train_pair.csv'), 'w') as outp:\n",
    "    writer = DictWriter(outp, fieldnames=['src_item_id', 'tgt_item_id', 'item_label'])\n",
    "    for line in inp:\n",
    "        row = json.loads(line)\n",
    "        writer.writerow(row)\n",
    "\n",
    "\n",
    "# with open(os.path.join(datafolder, 'item_valid_pair.jsonl'), 'r') as inp, open(os.path.join(datafolder, 'valid_pair.csv'), 'w') as outp:\n",
    "#     writer = DictWriter(outp, fieldnames=['src_item_id', 'tgt_item_id'])\n",
    "#     for line in inp:\n",
    "#         row = json.loads(line)\n",
    "#         writer.writerow(row)\n",
    "\n",
    "\n",
    "with open(os.path.join(datafolder, 'item_test_pair.jsonl'), 'r') as inp, open(os.path.join(datafolder, 'test_pair.csv'), 'w') as outp:\n",
    "    writer = DictWriter(outp, fieldnames=['src_item_id', 'tgt_item_id'])\n",
    "    for line in inp:\n",
    "        row = json.loads(line)\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd425f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将train和test的pair.jsonl文件转换成json文件\n",
    "\n",
    "datafolder = './data'\n",
    "train_item_pair = []\n",
    "with open(os.path.join(datafolder, 'item_train_pair.jsonl'), 'r', encoding='utf-8') as train_item_pair_jsonl:\n",
    "    with open(os.path.join(datafolder, 'train_pair.json'), 'w', encoding='utf-8') as train_item_pair_json:\n",
    "        for item in train_item_pair_jsonl:\n",
    "            item = json.loads(item)\n",
    "            train_item_pair.append(item)\n",
    "        json.dump(train_item_pair, train_item_pair_json, indent=4, ensure_ascii=False)\n",
    "        \n",
    "test_item_pair = []\n",
    "with open(os.path.join(datafolder, 'item_test_pair.jsonl'), 'r', encoding='utf-8') as test_item_pair_jsonl:\n",
    "    with open(os.path.join(datafolder, 'test_pair.json'), 'w', encoding='utf-8') as test_item_pair_json:\n",
    "        for item in test_item_pair_jsonl:\n",
    "            item = json.loads(item)\n",
    "            test_item_pair.append(item)\n",
    "        json.dump(test_item_pair, test_item_pair_json, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e5a318",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns1 = ['item_id', 'industry_name', 'cate_id', 'cate_name', 'cate_id_path', 'cate_name_path', 'item_image_name', 'title', 'item_pvs', 'sku_pvs']\n",
    "train_data = pd.read_csv('./data/train_info.csv', names=columns1)\n",
    "# valid_data = pd.read_csv('./data/valid_info.csv', names=columns1)\n",
    "test_data = pd.read_csv('./data/test_info.csv', names=columns1)\n",
    "\n",
    "columns2 = ['src_item_id', 'tgt_item_id', 'item_label']\n",
    "train_label = pd.read_csv('./data/train_pair.csv', names=columns2)\n",
    "# valid_label = pd.read_csv('./data/valid_pair.csv', names=columns2[:2])\n",
    "test_label = pd.read_csv('./data/test_pair.csv', names=columns2[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdc36a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e3143d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.fillna(\"\")\n",
    "train_data['item_pvs'] = '#' + train_data['item_pvs'] + '#;'\n",
    "train_data['sku_pvs'] = '#' + train_data['sku_pvs'] + '#;'\n",
    "\n",
    "# valid_data = valid_data.fillna(\"\")\n",
    "# valid_data['item_pvs'] = '#' + valid_data['item_pvs'] + '#;'\n",
    "# valid_data['sku_pvs'] = '#' + valid_data['sku_pvs'] + '#;'\n",
    "\n",
    "test_data = test_data.fillna(\"\")\n",
    "test_data['item_pvs'] = '#' + test_data['item_pvs'] + '#;'\n",
    "test_data['sku_pvs'] = '#' + test_data['sku_pvs'] + '#;'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5069087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去掉重复的'#'\n",
    "def clean_jing(df, column):\n",
    "    s = str(df[column])\n",
    "    s = re.sub('##+|#1#|#\\.#', '#', s)\n",
    "    s = re.sub(r'([^:;])#([^:;])', '\\g<1>\\g<2>', s)\n",
    "    s = re.sub(r'##', '# #', s)\n",
    "    s = re.sub(r':#;', ':# #;', s)\n",
    "    return s\n",
    " \n",
    "\n",
    "# 去除符号(最后弄)\n",
    "def clean_symbol(df, column):\n",
    "    s = str(df[column])\n",
    "    s = re.sub('#| |（|）|★|【|】|\\(|\\)|\\+|/|\\[|\\]|\\{|\\}|&|\\^(2|3)|°|\\$|@|!|！|✅|￥|？|《|》', '', s) # 去除一些符号\n",
    "    return s\n",
    "\n",
    "\n",
    "# 提取column的值\n",
    "def get_column(df, column):\n",
    "    s = df['item_pvs']\n",
    "    res = re.search(r'#' + column + '#:#(.+?)#;', s)\n",
    "    if res:\n",
    "        return res.group(1)\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "    \n",
    "# 去掉指定”描述“和”值“\n",
    "def clean_column(df, column_list):\n",
    "    s = df['item_pvs']\n",
    "    for column in column_list:\n",
    "        s = re.sub(r'#' + column + '#:#(.+?)#;', '', s)\n",
    "    return s\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a6493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "column_list = ['item_pvs', 'sku_pvs']\n",
    "for column in column_list:\n",
    "    train_data.loc[:, column] = train_data.apply(lambda x:clean_jing(x, column), axis=1)\n",
    "    # valid_data.loc[:, column] = valid_data.apply(lambda x:clean_jing(x, column), axis=1)\n",
    "    test_data.loc[:, column] = test_data.apply(lambda x:clean_jing(x, column), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec17db1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取train重复的描述\n",
    "# 37295\n",
    "def get_repeat_values(data):\n",
    "    pattern = re.compile(r'#(.+?)#')\n",
    "    repeat_list = []\n",
    "    num = 0\n",
    "    for item in tqdm(data):\n",
    "        #item = clean_jing(item)\n",
    "        res_list = pattern.findall(item)\n",
    "        rep_dict = dict(Counter(res_list[::2])) # 统计重复的\n",
    "        rep_list = sorted(rep_dict.items(), key=lambda d:d[1], reverse=True)\n",
    "        for element in rep_list:\n",
    "            if element[1] > 1:\n",
    "                repeat_list.append(element[0])\n",
    "                num += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    print(num)\n",
    "    return repeat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b36bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计 train 一个 item_pvs 中重复的“描述”频率\n",
    "list_rep_train_item = get_repeat_values(train_data['item_pvs'])\n",
    "dict_rep_train_item = dict(Counter(list_rep_train_item))\n",
    "sorted(dict_rep_train_item.items(), key=lambda d:d[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cefe69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 统计 valid 一个 item_pvs 中重复的“描述”频率\n",
    "# list_rep_valid_item = get_repeat_values(valid_data['item_pvs'])\n",
    "# dict_rep_valid_item = dict(Counter(list_rep_valid_item))\n",
    "# sorted(dict_rep_valid_item.items(), key=lambda d:d[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6b8f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计 test 一个 item_pvs 中重复的“描述”频率\n",
    "list_rep_test_item = get_repeat_values(test_data['item_pvs'])\n",
    "dict_rep_test_item = dict(Counter(list_rep_test_item))\n",
    "sorted(dict_rep_test_item.items(), key=lambda d:d[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c2eac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并train重复的”描述“的值\n",
    "def Merge_repeat(df, s, num):\n",
    "    item_pvs = df['item_pvs']\n",
    "    if item_pvs.count(s) > 1:\n",
    "        num[0] += 1\n",
    "        s_values = \"\"\n",
    "        res_list = re.findall(r''+ s +':#(.+?)#;', item_pvs) # 匹配出 重复的\n",
    "        for values in res_list:\n",
    "            s_values += (values + '、')\n",
    "\n",
    "        item_pvs = re.sub(r''+ s +':#(.+?)#;', '', item_pvs) # 将所有重复的替换为空\n",
    "\n",
    "        item_pvs += ';' + s + ':#' + s_values[:-1] + '#;' # 将重复的值连接后放置最后\n",
    "\n",
    "        item_pvs = re.sub(r';;', ';', item_pvs) # 替换 ;;\n",
    "    \n",
    "    return item_pvs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c9beaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = [0]\n",
    "rep_itempvs_list_train = sorted(dict_rep_train_item.items(), key=lambda d:d[1], reverse=True)\n",
    "for item in tqdm(rep_itempvs_list_train):\n",
    "    s = '#' + item[0] + '#'\n",
    "    train_data.loc[:, 'item_pvs'] = train_data.apply(lambda x:Merge_repeat(x, s, num), axis=1)\n",
    "\n",
    "num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb74012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 合并valid重复的”描述“的值\n",
    "\n",
    "# num = [0]\n",
    "# rep_itempvs_list_valid = sorted(dict_rep_valid_item.items(), key=lambda d:d[1], reverse=True)\n",
    "# for item in tqdm(rep_itempvs_list_valid):\n",
    "#     s = '#' + item[0] + '#'\n",
    "#     valid_data.loc[:, 'item_pvs'] = valid_data.apply(lambda x:Merge_repeat(x, s, num), axis=1)\n",
    "#\n",
    "# num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8b2994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并test重复的”描述“的值\n",
    "\n",
    "num = [0]\n",
    "rep_itempvs_list_test = sorted(dict_rep_test_item.items(), key=lambda d:d[1], reverse=True)\n",
    "for item in tqdm(rep_itempvs_list_test):\n",
    "    s = '#' + item[0] + '#'\n",
    "    test_data.loc[:, 'item_pvs'] = test_data.apply(lambda x:Merge_repeat(x, s, num), axis=1)\n",
    "    \n",
    "num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3990966",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 提出”品牌“，去掉指定值\n",
    "\n",
    "clean_column_list = ['品牌', '配送安装地区']\n",
    "column_list = ['brand', 'huohao', 'xinghao', 'chandi', 'caizhi', 'fengge']\n",
    "chinese_column_list = ['品牌', '货号', '型号', '产地', '材质', '风格']\n",
    "for i in tqdm(range(len(column_list))):\n",
    "    train_data.loc[:, column_list[i]] = train_data.apply(lambda x:get_column(x, chinese_column_list[i]), axis=1)\n",
    "    test_data.loc[:, column_list[i]] = test_data.apply(lambda x:get_column(x, chinese_column_list[i]), axis=1)\n",
    "    \n",
    "\n",
    "train_data.loc[:, 'item_pvs'] = train_data.apply(lambda x:clean_column(x, clean_column_list), axis=1)\n",
    "test_data.loc[:, 'item_pvs'] = test_data.apply(lambda x:clean_column(x, clean_column_list), axis=1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3e84ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 去除一些符号\n",
    "# 暂时不加上sku_pvs\n",
    "\n",
    "column_list = ['title', 'item_pvs', 'xinghao', 'huohao', 'chandi', 'caizhi', 'fengge']\n",
    "for column in tqdm(column_list):\n",
    "    train_data.loc[:, column] = train_data.apply(lambda x:clean_symbol(x, column), axis=1)\n",
    "    # valid_data.loc[:, column] = valid_data.apply(lambda x:clean_symbol(x, column), axis=1)\n",
    "    test_data.loc[:, column] = test_data.apply(lambda x:clean_symbol(x, column), axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0110a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train去除符号后存储成csv和json\n",
    "train_data.to_csv('./data/train_info_new_75_v3.csv', index=False)\n",
    "train_data.to_json('./data/train_info_new_75_v3.json', orient='records', force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9111e90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # valid去除符号后存储成csv和json\n",
    "# valid_data.to_csv('./data/valid_info_new_75_v3.csv', index=False)\n",
    "# valid_data.to_json('./data/valid_info_new_75_v3.json', orient='records', force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e919bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test去除符号后存储成csv和json\n",
    "test_data.to_csv('./data/test_info_new_81.csv', index=False)\n",
    "test_data.to_json('./data/test_info_new_81.json', orient='records', force_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
